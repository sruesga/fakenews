{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eee3408fe1ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (read only)\n",
    "\n",
    "\n",
    "**You can skip this section, however you may find these functions useful later in the assignment**\n",
    "\n",
    "We have provided this code so you see how the dataset was generated. You will have to come back some of these functions later in the assignment, so feel free to read through, to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize_sequence(tokenized):\n",
    "    return [w2i.get(w, unkI) for w in tokenized]\n",
    "def pad_sequence(numerized, pad_index, to_length):\n",
    "    pad = numerized[:to_length]\n",
    "    padded = pad + [pad_index] * (to_length - len(pad))\n",
    "    mask = [w != pad_index for w in padded]\n",
    "    return padded, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "for a in dataset:\n",
    "    a['tokenized'] = tokenizer.word_tokenizer(a['title'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "word_counts = Counter()\n",
    "for a in dataset:\n",
    "    word_counts.update(a['tokenized'])\n",
    "\n",
    "print(word_counts.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "# Creating the vocab\n",
    "vocab_size = 20000\n",
    "special_words = [\"<START>\", \"UNK\", \"PAD\"]\n",
    "vocabulary = special_words + [w for w, c in word_counts.most_common(vocab_size-len(special_words))]\n",
    "w2i = {w: i for i, w in enumerate(vocabulary)}\n",
    "\n",
    "# Numerizing and padding\n",
    "input_length = 20\n",
    "unkI, padI, startI = w2i['UNK'], w2i['PAD'], w2i['<START>']\n",
    "\n",
    "for a in dataset:\n",
    "    a['numerized'] = numerize_sequence(a['tokenized']) # Change words to IDs\n",
    "    a['numerized'], a['mask'] = pad_sequence(a['numerized'], padI, input_length) # Append appropriate PAD tokens\n",
    "    \n",
    "# Compute fraction of words that are UNK:\n",
    "word_counters = Counter([w for a in dataset for w in a['input'] if w != padI])\n",
    "\n",
    "print(\"Fraction of UNK words:\", float(word_counters[unkI]) / sum(word_counters.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "d_released_processed   = [d for d in dataset if d['cut'] != 'testing']\n",
    "d_unreleased_processed = [d for d in dataset if d['cut'] == 'testing']\n",
    "\n",
    "with open(\"dataset/headline_generation_dataset_processed.json\", \"w\") as f:\n",
    "    json.dump(d_released_processed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "d_released_processed   = [d for d in dataset if d['cut'] != 'testing']\n",
    "d_unreleased_processed = [d for d in dataset if d['cut'] == 'testing']\n",
    "\n",
    "with open(\"dataset/headline_generation_dataset_processed.json\", \"w\") as f:\n",
    "    json.dump(d_released_processed, f)\n",
    "\n",
    "# This file is purposefully left out of the assignment, we will use it to evaluate your model.\n",
    "with open(\"dataset/headline_generation_dataset_unreleased_processed.json\", \"w\") as f:\n",
    "    json.dump(d_unreleased_processed, f)\n",
    "    \n",
    "with open(\"dataset/headline_generation_vocabulary.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(vocabulary).encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size):\n",
    "    \"\"\" Builds a batch of source and target elements from the dataset.\n",
    "    \n",
    "        Arguments:\n",
    "            dataset: List[db_element] -- A list of dataset elements\n",
    "            batch_size: int -- The size of the batch that should be created\n",
    "        Returns:\n",
    "            batch_input: List[List[int]] -- List of source sequences\n",
    "            batch_target: List[List[int]] -- List of target sequences\n",
    "            batch_target_mask: List[List[int]] -- List of target batch masks\n",
    "    \"\"\"\n",
    "    \n",
    "    #####\n",
    "    # BEGIN YOUR CODE HERE \n",
    "    #####\n",
    "    \n",
    "    \n",
    "    # We get a list of indices we will choose from the dataset.\n",
    "    # The randint function uses a uniform distribution, giving equal probably to any entry\n",
    "    # for each batch\n",
    "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
    "    \n",
    "    # Recover what the entries for the batch are\n",
    "    batch = [dataset[i] for i in indices]\n",
    "    \n",
    "    # Get the raw numerized for this input, each element of the dataset has a 'numerized' key\n",
    "    batch_numerized = np.array([b['numerized'] for b in batch])\n",
    "\n",
    "    # Create an array of start_index that will be concatenated at position 1 for the input.\n",
    "    # Should be of shape (batch_size, 1)\n",
    "    start_tokens = np.full((batch_size, 1), start_index)\n",
    "\n",
    "    # Concatenate the start_tokens with the rest of the input\n",
    "    # The np.concatenate function should be useful\n",
    "    # The output should now be [batch_size, sequence_length+1]\n",
    "    batch_input = np.concatenate((start_tokens, batch_numerized), axis=1)\n",
    "\n",
    "    # Remove the last word from each element in the batch\n",
    "    # To restore the [batch_size, sequence_length] size\n",
    "    batch_input = batch_input[:, :-1]\n",
    "    \n",
    "    # The target should be the un-shifted numerized input\n",
    "    batch_target = batch_numerized\n",
    "\n",
    "    # The target-mask is a 0 or 1 filter to note which tokens are\n",
    "    # padding or not, to give the loss, so the model doesn't get rewarded for\n",
    "    # predicting PAD tokens.\n",
    "    batch_target_mask = np.array([a['mask'] for a in batch])\n",
    "    \n",
    "    #####\n",
    "    # END YOUR CODE HERE \n",
    "    #####\n",
    "        \n",
    "    return batch_input, batch_target, batch_target_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
